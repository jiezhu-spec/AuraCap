# Task Index 2026-02-21 ~ 2026-02-27

**Dominant tag**: work

## work (5)

- 2026-02-26 15:49:19 UTC | ### 核心内容

- **测试对象**：Qwen3.5 的 35B 和 27B 模型，均使用 FP8 精度。
- **硬件配置**：单卡 4090，显存 48G。
- **测试结果**：
  - **35B 模型**：
    - 单卡请求速度 120-130 时，200k 上下文仅能衰退到 90，性能不足。
  - **27B 模型**：
    - Dense 模型性能较好，输出稳定在 20t/s。
    - 200k 前缓冲 4k 输入时，保持 19t/s 左右，prefill 时间约 200 毫秒。
    - 使用 Agent Teams 并发，六个并发稳定输出 120t，任务完
- 2026-02-26 16:27:35 UTC | ### 核心内容
- **研究背景**：探讨了LLM（大语言模型）的Base与Instruct之间的权重相似性，定义了权重的sigma值。
- **关键数据**：Qwen 3.5 A3B的Base模型sigma值为 **0.01768391**，约为1%上下，表明相似性极高。
- **补充说明**：主流LLM的sigma值普遍较小，具体细节可参考P2论文。

### 行动项
- 进一步研究P2和P3论文中的相关细节。

### 标签
#Qwen3 #weight #LLM #HKU #相似性 #reasoning
- 2026-02-26 17:15:52 UTC | ### 核心内容

- **模型信息**  
  - 名称：Qwen3.5-35B-A3B  
  - 参数量：35亿  
  - 每个token活跃参数：30亿  
  - 量化：4-bit  
  - 磁盘占用：19.7GB  

- **运行环境**  
  - 硬件：单张3090显卡  
  - 上下文长度：从4K扩展到262K  
  - 零卸载，所有层运行在GPU上  
  - 性能：4K时112 tok/s，262K时114 tok/s，速度稳定  

- **默认设置问题**  
  - 默认设置在262K上下文时会OOM（内存溢出）  
  - 通过一个标志位解锁，具体解锁命
- 2026-02-26 17:25:47 UTC | ### 核心内容

#### 【核心引擎升级】
- **Mamba 前缀缓存**  
  - 使用参数：`--enable-prefix-caching --mamba-cache-mode align`  
  - 直接缓存 Mamba 状态，性能提升约 2 倍。

- **会话式流式输入**  
  - 新增 `StreamingInput API`  
  - 专为 ASR 等交互式场景设计。

- **异步调度 + 流水线并行**  
  - `--async-scheduling` 现已支持 Pipeline Parallelism。

---

#### 【NVIDIA Black
- 2026-02-26 21:42:20 UTC | ### 核心内容

1. **性能数据（tok/s）**：
   - **5090**：
     - 166 tok/s (z33b0t)
     - 153 tok/s (EmmanuelMr)
   - **4090**：
     - 122 tok/s (StubbyTech)
   - **3090**：
     - 112 tok/s (sudo)
     - 100 tok/s (Eduardo)
   - **6800XT**：
     - 20-30 tok/s (Dark)

2. **模型信息**：
   - 模型：Qwen3.5-35B-A3B
   - 量化

## technology (4)

- 2026-02-26 15:49:19 UTC | ### 核心内容

- **测试对象**：Qwen3.5 的 35B 和 27B 模型，均使用 FP8 精度。
- **硬件配置**：单卡 4090，显存 48G。
- **测试结果**：
  - **35B 模型**：
    - 单卡请求速度 120-130 时，200k 上下文仅能衰退到 90，性能不足。
  - **27B 模型**：
    - Dense 模型性能较好，输出稳定在 20t/s。
    - 200k 前缓冲 4k 输入时，保持 19t/s 左右，prefill 时间约 200 毫秒。
    - 使用 Agent Teams 并发，六个并发稳定输出 120t，任务完
- 2026-02-26 17:15:52 UTC | ### 核心内容

- **模型信息**  
  - 名称：Qwen3.5-35B-A3B  
  - 参数量：35亿  
  - 每个token活跃参数：30亿  
  - 量化：4-bit  
  - 磁盘占用：19.7GB  

- **运行环境**  
  - 硬件：单张3090显卡  
  - 上下文长度：从4K扩展到262K  
  - 零卸载，所有层运行在GPU上  
  - 性能：4K时112 tok/s，262K时114 tok/s，速度稳定  

- **默认设置问题**  
  - 默认设置在262K上下文时会OOM（内存溢出）  
  - 通过一个标志位解锁，具体解锁命
- 2026-02-26 17:25:47 UTC | ### 核心内容

#### 【核心引擎升级】
- **Mamba 前缀缓存**  
  - 使用参数：`--enable-prefix-caching --mamba-cache-mode align`  
  - 直接缓存 Mamba 状态，性能提升约 2 倍。

- **会话式流式输入**  
  - 新增 `StreamingInput API`  
  - 专为 ASR 等交互式场景设计。

- **异步调度 + 流水线并行**  
  - `--async-scheduling` 现已支持 Pipeline Parallelism。

---

#### 【NVIDIA Black
- 2026-02-26 21:42:20 UTC | ### 核心内容

1. **性能数据（tok/s）**：
   - **5090**：
     - 166 tok/s (z33b0t)
     - 153 tok/s (EmmanuelMr)
   - **4090**：
     - 122 tok/s (StubbyTech)
   - **3090**：
     - 112 tok/s (sudo)
     - 100 tok/s (Eduardo)
   - **6800XT**：
     - 20-30 tok/s (Dark)

2. **模型信息**：
   - 模型：Qwen3.5-35B-A3B
   - 量化

## learning (3)

- 2026-02-26 15:49:19 UTC | ### 核心内容

- **测试对象**：Qwen3.5 的 35B 和 27B 模型，均使用 FP8 精度。
- **硬件配置**：单卡 4090，显存 48G。
- **测试结果**：
  - **35B 模型**：
    - 单卡请求速度 120-130 时，200k 上下文仅能衰退到 90，性能不足。
  - **27B 模型**：
    - Dense 模型性能较好，输出稳定在 20t/s。
    - 200k 前缓冲 4k 输入时，保持 19t/s 左右，prefill 时间约 200 毫秒。
    - 使用 Agent Teams 并发，六个并发稳定输出 120t，任务完
- 2026-02-26 16:27:35 UTC | ### 核心内容
- **研究背景**：探讨了LLM（大语言模型）的Base与Instruct之间的权重相似性，定义了权重的sigma值。
- **关键数据**：Qwen 3.5 A3B的Base模型sigma值为 **0.01768391**，约为1%上下，表明相似性极高。
- **补充说明**：主流LLM的sigma值普遍较小，具体细节可参考P2论文。

### 行动项
- 进一步研究P2和P3论文中的相关细节。

### 标签
#Qwen3 #weight #LLM #HKU #相似性 #reasoning
- 2026-02-26 17:15:52 UTC | ### 核心内容

- **模型信息**  
  - 名称：Qwen3.5-35B-A3B  
  - 参数量：35亿  
  - 每个token活跃参数：30亿  
  - 量化：4-bit  
  - 磁盘占用：19.7GB  

- **运行环境**  
  - 硬件：单张3090显卡  
  - 上下文长度：从4K扩展到262K  
  - 零卸载，所有层运行在GPU上  
  - 性能：4K时112 tok/s，262K时114 tok/s，速度稳定  

- **默认设置问题**  
  - 默认设置在262K上下文时会OOM（内存溢出）  
  - 通过一个标志位解锁，具体解锁命
