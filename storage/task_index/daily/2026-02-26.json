{
  "window": "daily",
  "start": "2026-02-26",
  "end": "2026-02-26",
  "dominant_tag": "work",
  "tags": [
    {
      "tag": "work",
      "weight": 5,
      "entries": [
        {
          "entry_id": "entry-cc8bf6991ca147b9af5d7b09f1dfb7fa",
          "timestamp_display": "2026-02-26 15:49:19 UTC",
          "content_preview": "### 核心内容\n\n- **测试对象**：Qwen3.5 的 35B 和 27B 模型，均使用 FP8 精度。\n- **硬件配置**：单卡 4090，显存 48G。\n- **测试结果**：\n  - **35B 模型**：\n    - 单卡请求速度 120-130 时，200k 上下文仅能衰退到 90，性能不足。\n  - **27B 模型**：\n    - Dense 模型性能较好，输出稳定在 20t/s。\n    - 200k 前缓冲 4k 输入时，保持 19t/s 左右，prefill 时间约 200 毫秒。\n    - 使用 Agent Teams 并发，六个并发稳定输出 120t，任务完"
        },
        {
          "entry_id": "entry-29855104f239432c9e0e9735838e20b6",
          "timestamp_display": "2026-02-26 16:27:35 UTC",
          "content_preview": "### 核心内容\n- **研究背景**：探讨了LLM（大语言模型）的Base与Instruct之间的权重相似性，定义了权重的sigma值。\n- **关键数据**：Qwen 3.5 A3B的Base模型sigma值为 **0.01768391**，约为1%上下，表明相似性极高。\n- **补充说明**：主流LLM的sigma值普遍较小，具体细节可参考P2论文。\n\n### 行动项\n- 进一步研究P2和P3论文中的相关细节。\n\n### 标签\n#Qwen3 #weight #LLM #HKU #相似性 #reasoning"
        },
        {
          "entry_id": "entry-9853886539fd4fda8b5bcd9ba13906a6",
          "timestamp_display": "2026-02-26 17:15:52 UTC",
          "content_preview": "### 核心内容\n\n- **模型信息**  \n  - 名称：Qwen3.5-35B-A3B  \n  - 参数量：35亿  \n  - 每个token活跃参数：30亿  \n  - 量化：4-bit  \n  - 磁盘占用：19.7GB  \n\n- **运行环境**  \n  - 硬件：单张3090显卡  \n  - 上下文长度：从4K扩展到262K  \n  - 零卸载，所有层运行在GPU上  \n  - 性能：4K时112 tok/s，262K时114 tok/s，速度稳定  \n\n- **默认设置问题**  \n  - 默认设置在262K上下文时会OOM（内存溢出）  \n  - 通过一个标志位解锁，具体解锁命"
        },
        {
          "entry_id": "entry-076c16f469f643f791727ed5a38dc5fd",
          "timestamp_display": "2026-02-26 17:25:47 UTC",
          "content_preview": "### 核心内容\n\n#### 【核心引擎升级】\n- **Mamba 前缀缓存**  \n  - 使用参数：`--enable-prefix-caching --mamba-cache-mode align`  \n  - 直接缓存 Mamba 状态，性能提升约 2 倍。\n\n- **会话式流式输入**  \n  - 新增 `StreamingInput API`  \n  - 专为 ASR 等交互式场景设计。\n\n- **异步调度 + 流水线并行**  \n  - `--async-scheduling` 现已支持 Pipeline Parallelism。\n\n---\n\n#### 【NVIDIA Black"
        },
        {
          "entry_id": "entry-48eeec1a6b234caf976f85db50068932",
          "timestamp_display": "2026-02-26 21:42:20 UTC",
          "content_preview": "### 核心内容\n\n1. **性能数据（tok/s）**：\n   - **5090**：\n     - 166 tok/s (z33b0t)\n     - 153 tok/s (EmmanuelMr)\n   - **4090**：\n     - 122 tok/s (StubbyTech)\n   - **3090**：\n     - 112 tok/s (sudo)\n     - 100 tok/s (Eduardo)\n   - **6800XT**：\n     - 20-30 tok/s (Dark)\n\n2. **模型信息**：\n   - 模型：Qwen3.5-35B-A3B\n   - 量化"
        }
      ]
    },
    {
      "tag": "technology",
      "weight": 4,
      "entries": [
        {
          "entry_id": "entry-cc8bf6991ca147b9af5d7b09f1dfb7fa",
          "timestamp_display": "2026-02-26 15:49:19 UTC",
          "content_preview": "### 核心内容\n\n- **测试对象**：Qwen3.5 的 35B 和 27B 模型，均使用 FP8 精度。\n- **硬件配置**：单卡 4090，显存 48G。\n- **测试结果**：\n  - **35B 模型**：\n    - 单卡请求速度 120-130 时，200k 上下文仅能衰退到 90，性能不足。\n  - **27B 模型**：\n    - Dense 模型性能较好，输出稳定在 20t/s。\n    - 200k 前缓冲 4k 输入时，保持 19t/s 左右，prefill 时间约 200 毫秒。\n    - 使用 Agent Teams 并发，六个并发稳定输出 120t，任务完"
        },
        {
          "entry_id": "entry-9853886539fd4fda8b5bcd9ba13906a6",
          "timestamp_display": "2026-02-26 17:15:52 UTC",
          "content_preview": "### 核心内容\n\n- **模型信息**  \n  - 名称：Qwen3.5-35B-A3B  \n  - 参数量：35亿  \n  - 每个token活跃参数：30亿  \n  - 量化：4-bit  \n  - 磁盘占用：19.7GB  \n\n- **运行环境**  \n  - 硬件：单张3090显卡  \n  - 上下文长度：从4K扩展到262K  \n  - 零卸载，所有层运行在GPU上  \n  - 性能：4K时112 tok/s，262K时114 tok/s，速度稳定  \n\n- **默认设置问题**  \n  - 默认设置在262K上下文时会OOM（内存溢出）  \n  - 通过一个标志位解锁，具体解锁命"
        },
        {
          "entry_id": "entry-076c16f469f643f791727ed5a38dc5fd",
          "timestamp_display": "2026-02-26 17:25:47 UTC",
          "content_preview": "### 核心内容\n\n#### 【核心引擎升级】\n- **Mamba 前缀缓存**  \n  - 使用参数：`--enable-prefix-caching --mamba-cache-mode align`  \n  - 直接缓存 Mamba 状态，性能提升约 2 倍。\n\n- **会话式流式输入**  \n  - 新增 `StreamingInput API`  \n  - 专为 ASR 等交互式场景设计。\n\n- **异步调度 + 流水线并行**  \n  - `--async-scheduling` 现已支持 Pipeline Parallelism。\n\n---\n\n#### 【NVIDIA Black"
        },
        {
          "entry_id": "entry-48eeec1a6b234caf976f85db50068932",
          "timestamp_display": "2026-02-26 21:42:20 UTC",
          "content_preview": "### 核心内容\n\n1. **性能数据（tok/s）**：\n   - **5090**：\n     - 166 tok/s (z33b0t)\n     - 153 tok/s (EmmanuelMr)\n   - **4090**：\n     - 122 tok/s (StubbyTech)\n   - **3090**：\n     - 112 tok/s (sudo)\n     - 100 tok/s (Eduardo)\n   - **6800XT**：\n     - 20-30 tok/s (Dark)\n\n2. **模型信息**：\n   - 模型：Qwen3.5-35B-A3B\n   - 量化"
        }
      ]
    },
    {
      "tag": "learning",
      "weight": 3,
      "entries": [
        {
          "entry_id": "entry-cc8bf6991ca147b9af5d7b09f1dfb7fa",
          "timestamp_display": "2026-02-26 15:49:19 UTC",
          "content_preview": "### 核心内容\n\n- **测试对象**：Qwen3.5 的 35B 和 27B 模型，均使用 FP8 精度。\n- **硬件配置**：单卡 4090，显存 48G。\n- **测试结果**：\n  - **35B 模型**：\n    - 单卡请求速度 120-130 时，200k 上下文仅能衰退到 90，性能不足。\n  - **27B 模型**：\n    - Dense 模型性能较好，输出稳定在 20t/s。\n    - 200k 前缓冲 4k 输入时，保持 19t/s 左右，prefill 时间约 200 毫秒。\n    - 使用 Agent Teams 并发，六个并发稳定输出 120t，任务完"
        },
        {
          "entry_id": "entry-29855104f239432c9e0e9735838e20b6",
          "timestamp_display": "2026-02-26 16:27:35 UTC",
          "content_preview": "### 核心内容\n- **研究背景**：探讨了LLM（大语言模型）的Base与Instruct之间的权重相似性，定义了权重的sigma值。\n- **关键数据**：Qwen 3.5 A3B的Base模型sigma值为 **0.01768391**，约为1%上下，表明相似性极高。\n- **补充说明**：主流LLM的sigma值普遍较小，具体细节可参考P2论文。\n\n### 行动项\n- 进一步研究P2和P3论文中的相关细节。\n\n### 标签\n#Qwen3 #weight #LLM #HKU #相似性 #reasoning"
        },
        {
          "entry_id": "entry-9853886539fd4fda8b5bcd9ba13906a6",
          "timestamp_display": "2026-02-26 17:15:52 UTC",
          "content_preview": "### 核心内容\n\n- **模型信息**  \n  - 名称：Qwen3.5-35B-A3B  \n  - 参数量：35亿  \n  - 每个token活跃参数：30亿  \n  - 量化：4-bit  \n  - 磁盘占用：19.7GB  \n\n- **运行环境**  \n  - 硬件：单张3090显卡  \n  - 上下文长度：从4K扩展到262K  \n  - 零卸载，所有层运行在GPU上  \n  - 性能：4K时112 tok/s，262K时114 tok/s，速度稳定  \n\n- **默认设置问题**  \n  - 默认设置在262K上下文时会OOM（内存溢出）  \n  - 通过一个标志位解锁，具体解锁命"
        }
      ]
    }
  ],
  "untagged_entries": [],
  "generated_at": "2026-02-27T10:09:47.722170+00:00"
}